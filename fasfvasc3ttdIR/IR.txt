Practical 1



def intersect(p1, p2):
 i, j = 0, 0
 answer = []
 while i < len(p1) and j < len(p2):
 if p1[i] == p2[j]:
 answer.append(p1[i])
 i += 1
 j += 1
 elif p1[i]< p2[j]:
 i += 1
 else:
 j += 1
 return answer
print(intersect([1, 2, 3, 11, 21, 31, 56], [3, 6, 9, 11, 31, 40, 46, 50]))



Practical 2



package javaapplication3;



import java.util.Scanner;

public class JavaApplication3 {

    int path[][] = new int[10][10];
    double pagerank[] = new double[10];
    public void calc(double totalNodes) {
        double InitialPageRank;
        double OutgoingLinks = 0;
        double DampingFactor = 0.85;
        double TempPageRank[] = new double[10];
        int ExternalNodeNumber;
        int InternalNodeNumber;
        int k = 1;
        int ITERATION_STEP = 1;
        InitialPageRank = 1 / totalNodes;
        System.out.println("Total Number of Nodes :" + totalNodes
                + "\t Initial PageRank of all Nodes :" + InitialPageRank + "\n");
        for (k = 1; k <= totalNodes; k++) {
            this.pagerank[k] = InitialPageRank;
        }
        System.out.println("\n Initial PageRank Values, 0th Step \n");
        for (k = 1; k <= totalNodes; k++) {
            System.out.println(" Page Rank of " + k + " is :\t" + this.pagerank[k] + "\n");
        }
        while (ITERATION_STEP <= 2) {
            for (k = 1; k <= totalNodes; k++) {
                TempPageRank[k] = this.pagerank[k];
                this.pagerank[k] = 0;
            }
            for (InternalNodeNumber = 1; InternalNodeNumber <= totalNodes; InternalNodeNumber++) {
                for (ExternalNodeNumber = 1; ExternalNodeNumber <= totalNodes; ExternalNodeNumber++) {
                    if (this.path[ExternalNodeNumber][InternalNodeNumber] == 1) {
                        k = 1;
                        OutgoingLinks = 0;
                        while (k <= totalNodes) {
                            if (this.path[ExternalNodeNumber][k] == 1) {
                                OutgoingLinks = OutgoingLinks + 1;
                            }
                            k = k + 1;
                        }
                        this.pagerank[InternalNodeNumber] += TempPageRank[ExternalNodeNumber]
                                * (1 / OutgoingLinks);
                    }
                }
            }
            System.out.println("\n After " + ITERATION_STEP + "th Step \n");
            for (k = 1; k <= totalNodes; k++) {
                System.out.println("Page Rank of " + k + "is \t" + this.pagerank[k] + "\n");
            }
            ITERATION_STEP = ITERATION_STEP + 1;
        }
        for (k = 1; k <= totalNodes; k++) {
            this.pagerank[k] = (1 - DampingFactor) + DampingFactor * this.pagerank[k];
        }
        System.out.println("\n Final Page Rank : \n");
        for (k = 1; k <= totalNodes; k++) {
            System.out.println("Page rank of" + k + "is :\t" + this.pagerank[k] + "\n");
        }

    }

    public static void main(String[] args) {
        int nodes, i, j;
        Scanner in = new Scanner(System.in);
        System.out.println("Enter the Number of WebPages \n");
        nodes = in.nextInt();
        JavaApplication3 p = new JavaApplication3();
        System.out.println("enter the Adjacency Matrix with 1 -> PATH & 0 -> NO PATH Between two WebPages: \n");
        for (i = 1; i <= nodes; i++) {
            for (j = 1; j <= nodes; j++) {
                p.path[i][j] = in.nextInt();
                if (j == i) {
                    p.path[i][j] = 0;
                }
            }
        }
        p.calc(nodes);
    }
}




Practical 3



def editDistance(s1,s2,m,n):
 if m==0:
 return n
 if n==0:
 return m
 if(s1[m-1]==s2[n-1]):
 return editDistance(s1,s2,m-1,n-1)
 return 1+min(editDistance(s1,s2,m,n-1),editDistance(s1,s2,m1,n),editDistance(s1,s2,m-1,n-1))
s1="network"
s2="cats"
print(editDistance(s1,s2,len(s1),len(s2)))



Practcial 4



package javaapplication2;

import java.io.BufferedReader;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.IOException;

public class JavaApplication2 {

    public static void main(String[] args) throws FileNotFoundException, IOException {
        BufferedReader reader1 = new BufferedReader(new FileReader("C://Users//chira//Desktop//Code//IR//file1.txt"));
        BufferedReader reader2 = new BufferedReader(new FileReader("C://Users//chira//Desktop//Code//IR//file2.txt"));
        String line1 = reader1.readLine();
        String line2 = reader2.readLine();
        boolean areEqual = true;
        int lineNum = 1;
        while (line1 != null || line2 != null) {
            if (line1 == null || line2 == null) {
                areEqual = false;
                break;
            } else if (!line1.equalsIgnoreCase(line2)) {
                areEqual = false;
                break;
            }
            line1 = reader1.readLine();
            line2 = reader2.readLine();
            lineNum++;
        }
        if (areEqual) {
            System.out.println("Two files have same content.");
        } else {
            System.out.println("Two files have different content. They differ at line " + lineNum);
            System.out.println("File1 has " + line1 + " and File2 has " + line2 + " at line " + lineNum);
        }
        reader1.close();
        reader2.close();
    }
}




Practical 5



import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
eg = input("Enter any sentence: ")
stop_words = set(stopwords.words("english"))
tokens=word_tokenize(eg)
filtered=[]
for w in tokens:
 if w not in stop_words:
 filtered.append(w)
print(tokens)
print(filtered)

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
eg = open("text.txt", "r")
print(eg)
for line in eg:
 print(line)
 
stop_words = set(stopwords.words("english"))
tokens=word_tokenize(line)
filtered=[]
for w in tokens:
 if w not in stop_words:
 filtered.append(w)
print(tokens)
print(filtered)




Practical 6



import re
import tweepy
from tweepy import OAuthHandler
from textblob import TextBlob


class TwitterClient(object):
    def __init__(self):
        consumer_key='LlnXUxe64nBJQs0vCFPTj6Vkx'
        consumer_secret='PyhmahYOAONBaDBdw5U2mnIXCw6XdGKza0kgw35Dl86du5EegD'
        access_token='4313447914-5qh3YuUbTegPvjTMhfmeQONudsQgBDVJtHuijHB'
        access_token_secret='DBizwMGRuSemXtq3HbOaatBHa7rFXUywofhwYQY91Zj9N'
        
        try:
            self.auth = OAuthHandler(consumer_key, consumer_secret)
            self.auth.set_access_token(access_token, access_token_secret)
            self.api = tweepy.API(self.auth)
   
        except:
            print("Error: Authentication Failed")
    
    def clean_tweet(self, tweet):
        return ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\s+)"," ", tweet).split())
    
    def get_tweet_sentiment(self, tweet):
        analysis = TextBlob(self.clean_tweet(tweet))
        if analysis.sentiment.polarity > 0:
            return 'positive'
        else:
            return 'negative'
    
    def get_tweets(self, query, count = 10):
        tweets = []
        
        try:
            fetched_tweets = self.api.search_tweets(q = query, count = count)
            for tweet in fetched_tweets:
                parsed_tweet = {}
                parsed_tweet['text'] = tweet.text
                parsed_tweet['sentiment'] = self.get_tweet_sentiment (tweet. text)
                if tweet.retweet_count > 0:
                    if parsed_tweet not in tweets:
                        tweets.append(parsed_tweet)
                else:
                    tweets. append(parsed_tweet)
            return tweets
        except tweepy.TweepyException as e:
            print("Error : " + str(e))
    
def main():
    api = TwitterClient()
    tweets = api.get_tweets(query = 'Corona Virus', count = 50)
#     print(api)
    ptweets = [tweet for tweet in tweets if tweet['sentiment'] == 'positive'] 
    print("Positive tweets percentage: {} %".format(100*len(ptweets)/len(tweets)))
    ntweets = [tweet for tweet in tweets if tweet['sentiment'] == 'negative']
    print("Negative tweets percentage: {} %".format(100*len(ntweets)/len(tweets)))
    print("\n\nPositive tweets:")
    for tweet in ptweets[:5]:
        print(tweet['text'])
    print("\n\nNegative tweets:")
    for tweet in ntweets[:5]:
        print (tweet[ 'text' ])
if __name__ == "__main__":
    main()



Practical 7



import requests
from parsel import Selector
import time
start=time.time()
response=requests.get('http://recurship.com/')
selector=Selector(response.text)
href_links=selector.xpath('//a/@href').getall()
image_links=selector.xpath('//img/@src').getall()
print("**************Href_links***********************")
print(href_links)
print("******************/href_links*****************") # data type of this 
is Lists
print("**************Image_links***********************")
print(image_links)
print("******************/image_links*****************")
end=time.time()
print("Time Taken in seconds: ",(end-start))



Practical 8



import csv
import requests
import xml.etree.ElementTree as ET
def loadRSS():
 url = "https://timesofindia.indiatimes.com/rssfeeds/1081479906.cms"
 resp = requests.get(url)
 with open('topnewsfeed.xml','wb') as f:
 f.write(resp.content)
def parseXML(xmlfile):
 tree = ET.parse(xmlfile)
 root = tree.getroot()
 newsitems = []
 for item in root.findall("./channel/item"):
 news = {}
 for child in item:
 news[child.tag] = child.text.encode('utf8')
 newsitems.append(news)
 return newsitems
22
def savetoCSV(newsitems, filename):
 fields = ['guid', 'title', 'pubDate', 'description', 'link']
 with open(filename, 'w') as csvfile:
 writer = csv.DictWriter(csvfile, fieldnames = fields)
 writer.writeheader()
 writer.writerows(newsitems)
loadRSS()
newsitems = parseXML('topnewsfeed.xml')
savetoCSV(newsitems, 'topnews.csv')










