{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bitwise Operation\n",
    "def intersect(p1, p2):\n",
    "    i, j = 0, 0\n",
    "    answer = []\n",
    "    while i < len(p1) and j < len(p2):\n",
    "        if p1[i] == p2[j]:\n",
    "            answer.append(p1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif p1[i]< p2[j]:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return answer\n",
    "\n",
    "print(intersect([1, 2, 3, 11, 21, 31, 56], [3, 6, 9, 11, 31, 40, 46, 50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit Distance\n",
    "\n",
    "def editDistance(s1,s2,m,n):\n",
    "    if m==0:\n",
    "        return n\n",
    "    if n==0:\n",
    "        return m\n",
    "    if(s1[m-1]==s2[n-1]):\n",
    "        return editDistance(s1,s2,m-1,n-1)\n",
    "    return 1+min(editDistance(s1,s2,m,n-1),editDistance(s1,s2,m-1,n),editDistance(s1,s2,m-1,n-1))\n",
    "s1=\"network\"\n",
    "s2=\"cats\"\n",
    "editDistance(s1,s2,len(s1),len(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing of a Text Document\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "eg = input(\"Enter any sentence\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tokens=word_tokenize(eg)\n",
    "\n",
    "filtered=[]\n",
    "for w in tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered.append(w)\n",
    "\n",
    "print(tokens)\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing of a Text Document #2\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "eg = open(\"LoremIpsum.txt\", \"r\")\n",
    "print(eg)\n",
    "for line in eg:\n",
    "    print(line)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tokens=word_tokenize(line)\n",
    "filtered=[]\n",
    "for w in tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered.append(w)\n",
    "\n",
    "print(tokens)\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter \n",
    "# https://apps.twitter.com\n",
    "import re\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "class TwitterClient(object):\n",
    "    def __init__(self):\n",
    "        consumer_key='LlnXUxe64nBJQs0vCFPTj6Vkx'\n",
    "        consumer_secret='PyhmahYOAONBaDBdw5U2mnIXCw6XdGKza0kgw35Dl86du5EegD'\n",
    "        access_token='4313447914-5qh3YuUbTegPvjTMhfmeQONudsQgBDVJtHuijHB'\n",
    "        access_token_secret='DBizwMGRuSemXtq3HbOaatBHa7rFXUywofhwYQY91Zj9N'\n",
    "        \n",
    "        try:\n",
    "            self.auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "            self.auth.set_access_token(access_token, access_token_secret)\n",
    "            self.api = tweepy.API(self.auth)\n",
    "   \n",
    "        except:\n",
    "            print(\"Error: Authentication Failed\")\n",
    "    \n",
    "    def clean_tweet(self, tweet):\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\s+)\",\" \", tweet).split())\n",
    "    \n",
    "    def get_tweet_sentiment(self, tweet):\n",
    "        analysis = TextBlob(self.clean_tweet(tweet))\n",
    "        if analysis.sentiment.polarity > 0:\n",
    "            return 'positive'\n",
    "        else:\n",
    "            return 'negative'\n",
    "    \n",
    "    def get_tweets(self, query, count = 10):\n",
    "        tweets = []\n",
    "        \n",
    "        try:\n",
    "            fetched_tweets = self.api.search_tweets(q = query, count = count)\n",
    "            for tweet in fetched_tweets:\n",
    "                parsed_tweet = {}\n",
    "                parsed_tweet['text'] = tweet.text\n",
    "                parsed_tweet['sentiment'] = self.get_tweet_sentiment (tweet. text)\n",
    "                if tweet.retweet_count > 0:\n",
    "                    if parsed_tweet not in tweets:\n",
    "                        tweets.append(parsed_tweet)\n",
    "                else:\n",
    "                    tweets. append(parsed_tweet)\n",
    "            return tweets\n",
    "        except tweepy.TweepyException as e:\n",
    "            print(\"Error : \" + str(e))\n",
    "    \n",
    "def main():\n",
    "    api = TwitterClient()\n",
    "    tweets = api.get_tweets(query = 'Corona Virus', count = 50)\n",
    "#     print(api)\n",
    "    ptweets = [tweet for tweet in tweets if tweet['sentiment'] == 'positive'] \n",
    "    print(\"Positive tweets percentage: {} %\".format(100*len(ptweets)/len(tweets)))\n",
    "    ntweets = [tweet for tweet in tweets if tweet['sentiment'] == 'negative']\n",
    "    print(\"Negative tweets percentage: {} %\".format(100*len(ntweets)/len(tweets)))\n",
    "    print(\"\\n\\nPositive tweets:\")\n",
    "    for tweet in ptweets[:5]:\n",
    "        print(tweet['text'])\n",
    "    print(\"\\n\\nNegative tweets:\")\n",
    "    for tweet in ntweets[:5]:\n",
    "        print (tweet[ 'text' ])\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Crawler\n",
    "import requests \n",
    "from parsel import Selector\n",
    "import time\n",
    "start = time.time()\n",
    "response = requests.get('https://www.fcbarcelona.com')\n",
    "selector = Selector(response.text)\n",
    "href_links = selector.xpath('//a/@href').getall()\n",
    "image_links = selector.xpath('//img/@src').getall()\n",
    "print(\"***************Href_links***************\")\n",
    "print(href_links)\n",
    "print(\"***************Image_links***************\")\n",
    "print(image_links)\n",
    "end = time.time()\n",
    "print(\"***************Time_Taken***************\")\n",
    "print(\"Time taken in seconds: \", (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RSS\n",
    "import csv\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def loadRSS():\n",
    "    url = \"https://timesofindia.indiatimes.com/rssfeeds/1081479906.cms\"\n",
    "    resp = requests.get(url)\n",
    "    with open('topnewsfeed.xml','wb') as f:\n",
    "        f.write(resp.content)\n",
    "\n",
    "def parseXML(xmlfile):\n",
    "    tree = ET.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    newsitems = []\n",
    "    for item in root.findall(\"./channel/item\"):\n",
    "        news = {}\n",
    "        for child in item:\n",
    "            news[child.tag] = child.text.encode('utf8')\n",
    "        newsitems.append(news)\n",
    "    return newsitems\n",
    "\n",
    "def savetoCSV(newsitems, filename):\n",
    "    fields = ['guid', 'title', 'pubDate', 'description', 'link']\n",
    "    with open(filename, 'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames = fields)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(newsitems)\n",
    "\n",
    "loadRSS()\n",
    "newsitems = parseXML('topnewsfeed.xml')\n",
    "savetoCSV(newsitems, 'topnews.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
